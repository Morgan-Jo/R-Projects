setwd("~/Work/Github/R - Project/Wine Quality")
# Load libraries
library(ggplot2)
library(caret)
library(randomForest)
library(corrplot)
library(dplyr)
# Load datasets
red <- read.csv("winequality-red.csv", sep = ";")
white <- read.csv("winequality-white.csv", sep = ";")
# Add wine type
red$type <- 'red'
white$type <- 'white'
# Combine datasets
wine <- rbind(red, white)
# Convert type to factor
wine$type <- as.factor(wine$type)
# 1. EDA: Summary stats
summary(wine)
# 2. Correlation matrix
cor_matrix <- cor(wine[, 1:11])
corrplot(cor_matrix, method = "color", tl.cex = 0.8)
# 3. Visualize quality distribution
ggplot(wine, aes(x = quality, fill = type)) +
geom_bar(position = "dodge") +
labs(title = "Wine Quality Distribution")
# 4. Relationship: Alcohol vs. Quality
ggplot(wine, aes(x = alcohol, y = quality, color = type)) +
geom_point(alpha = 0.4) +
geom_smooth(method = "lm") +
labs(title = "Alcohol vs Quality")
# 5. Prepare data
set.seed(123)
wine$quality <- as.factor(wine$quality)  # Classification
trainIndex <- createDataPartition(wine$quality, p = 0.8, list = FALSE)
train <- wine[trainIndex, ]
test <- wine[-trainIndex, ]
# 6. Random Forest Classifier
rf_model <- randomForest(quality ~ ., data = train[, -which(names(train) == "type")], ntree = 100)
pred_rf <- predict(rf_model, test)
# Confusion Matrix
confusionMatrix(pred_rf, test$quality)
# Feature Importance
importance <- importance(rf_model)
varImpPlot(rf_model, main = "Feature Importance - Random Forest")
# 7. Linear Regression (for regression prediction)
wine$quality <- as.numeric(as.character(wine$quality))
lm_model <- lm(quality ~ . -type, data = train)
summary(lm_model)
# Train-test split
set.seed(123)
index <- sample(1:nrow(wine), 0.8 * nrow(wine))
train <- wine[index, ]
test <- wine[-index, ]
# Fit Linear Regression Model
lm_model <- lm(quality ~ . -type, data = train)
summary(lm_model)
# Predict and evaluate
lm_pred <- predict(lm_model, test)
rmse <- sqrt(mean((lm_pred - test$quality)^2))
print(paste("RMSE of Linear Regression:", round(rmse, 3)))
# Load libraries
library(ggplot2)
library(caret)
library(randomForest)
library(corrplot)
library(dplyr)
# Load datasets
red <- read.csv("winequality-red.csv", sep = ";")
white <- read.csv("winequality-white.csv", sep = ";")
# Add wine type
red$type <- 'red'
white$type <- 'white'
# Combine datasets
wine <- rbind(red, white)
# Convert type to factor
wine$type <- as.factor(wine$type)
# 1. EDA: Summary stats
summary(wine)
# 2. Correlation matrix
cor_matrix <- cor(wine[, 1:11])
corrplot(cor_matrix, method = "color", tl.cex = 0.8)
# 3. Visualize quality distribution
ggplot(wine, aes(x = quality, fill = type)) +
geom_bar(position = "dodge") +
labs(title = "Wine Quality Distribution")
# 4. Relationship: Alcohol vs. Quality
ggplot(wine, aes(x = alcohol, y = quality, color = type)) +
geom_point(alpha = 0.4) +
geom_smooth(method = "lm") +
labs(title = "Alcohol vs Quality")
# 5. Prepare data
set.seed(123)
wine$quality <- as.factor(wine$quality)  # Classification
trainIndex <- createDataPartition(wine$quality, p = 0.8, list = FALSE)
train <- wine[trainIndex, ]
test <- wine[-trainIndex, ]
# 6. Random Forest Classifier
rf_model <- randomForest(quality ~ ., data = train[, -which(names(train) == "type")], ntree = 100)
pred_rf <- predict(rf_model, test)
# Confusion Matrix
confusionMatrix(pred_rf, test$quality)
# Feature Importance
importance <- importance(rf_model)
varImpPlot(rf_model, main = "Feature Importance - Random Forest")
# 7. Linear Regression (for regression prediction)
wine$quality <- as.numeric(as.character(wine$quality))
# Train-test split
set.seed(123)
index <- sample(1:nrow(wine), 0.8 * nrow(wine))
train <- wine[index, ]
test <- wine[-index, ]
# Fit Linear Regression Model
lm_model <- lm(quality ~ . -type, data = train)
summary(lm_model)
# Predict and evaluate
lm_pred <- predict(lm_model, test)
rmse <- sqrt(mean((lm_pred - test$quality)^2))
print(paste("RMSE of Linear Regression:", round(rmse, 3)))
setwd("~/Work/Github/R - Project/Student Performance")
# Load libraries
library(tidyverse)
library(caret)
library(corrr)
library(ggplot2)
library(gridExtra)
# Load datasets
math <- read.csv("student-mat.csv", sep = ";")
por <- read.csv("student-por.csv", sep = ";")
# Add subject label
math$subject <- "Math"
por$subject <- "Portuguese"
# Combine datasets
df <- bind_rows(math, por)
# Summary: Count by subject
table(df$subject)
# ---------------------------------------------
# 1. Compare Distributions: G3 by Subject
# ---------------------------------------------
ggplot(df, aes(x = G3, fill = subject)) +
geom_histogram(binwidth = 1, position = "dodge", color = "white") +
labs(title = "Final Grades Distribution by Subject", x = "G3 (Final Grade)", y = "Count")
# Study time comparison
ggplot(df, aes(x = factor(studytime), fill = subject)) +
geom_bar(position = "dodge") +
labs(title = "Study Time by Subject", x = "Study Time Level", y = "Number of Students")
# Absences comparison
ggplot(df, aes(x = absences, fill = subject)) +
geom_histogram(position = "identity", alpha = 0.5, binwidth = 2) +
labs(title = "Absences Distribution by Subject", x = "Absences", y = "Count")
# ---------------------------------------------
# 2. Correlation and Feature Importance by Subject
# ---------------------------------------------
# Split into math and portuguese again
math_data <- df %>% filter(subject == "Math")
por_data <- df %>% filter(subject == "Portuguese")
# Correlation matrices
math_cor <- correlate(select_if(math_data, is.numeric))
por_cor <- correlate(select_if(por_data, is.numeric))
# Correlation with G3
math_cor %>% focus(G3) %>% arrange(desc(abs(G3)))
por_cor %>% focus(G3) %>% arrange(desc(abs(G3)))
# ---------------------------------------------
# 3. Linear Regression Comparison
# ---------------------------------------------
# Prepare formula and features
features <- c("studytime", "absences", "failures", "Medu", "Fedu")
formula <- as.formula(paste("G3 ~", paste(features, collapse = " + ")))
# Train/test split function
split_data <- function(data) {
set.seed(123)
idx <- createDataPartition(data$G3, p = 0.8, list = FALSE)
list(train = data[idx, ], test = data[-idx, ])
}
math_split <- split_data(math_data)
por_split <- split_data(por_data)
# Train models
lm_math <- train(formula, data = math_split$train, method = "lm")
lm_por <- train(formula, data = por_split$train, method = "lm")
# Evaluate models
res_math <- postResample(predict(lm_math, math_split$test), math_split$test$G3)
res_por <- postResample(predict(lm_por, por_split$test), por_split$test$G3)
# Show results
print("Math Linear Regression Performance:")
print(res_math)
print("Portuguese Linear Regression Performance:")
print(res_por)
# ---------------------------------------------
# 4. Logistic Regression: Predict Pass/Fail
# ---------------------------------------------
df$pass <- as.factor(ifelse(df$G3 >= 10, "yes", "no"))
# Logistic model per subject
log_math <- train(pass ~ studytime + absences + failures + Medu + Fedu,
data = math_data, method = "glm", family = "binomial")
